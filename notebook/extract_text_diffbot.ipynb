{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Database setup\n",
    "\n",
    "Connect to the database, extract the necessary data and close the connection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of posts to process: 53\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "\n",
    "conn = psycopg2.connect(\"dbname=hn user=julien\")\n",
    "cur = conn.cursor()\n",
    "\n",
    "cur.execute(\"\"\"\n",
    "SELECT\n",
    "\thn_post.id,\n",
    "\thn_post.url\n",
    "FROM\n",
    "\thn_post\n",
    "\tLEFT JOIN hn_article ON hn_post.id = hn_article.id -- We do a left join to get all posts\n",
    "WHERE\n",
    "\thn_article IS NULL -- only get posts that have not been processed yet\n",
    "\tAND hn_post.score > 100 -- only get posts with a score higher than 700\n",
    "    AND (hn_post.url IS NOT NULL); -- only get posts with a url\n",
    "\"\"\")\n",
    "\n",
    "posts = cur.fetchall()\n",
    "\n",
    "print(\"Number of posts to process: {}\".format(len(posts)))\n",
    "\n",
    "cur.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text extraction\n",
    "Using the Diffbot API, extract the text from the URLs and save it in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unclosed client session\n",
      "client_session: <aiohttp.client.ClientSession object at 0x1055521d0>\n"
     ]
    }
   ],
   "source": [
    "from os import getenv\n",
    "import aiohttp\n",
    "from dotenv import load_dotenv\n",
    "from urllib.parse import urlparse, quote\n",
    "from fitz import open as open_pdf\n",
    "from yarl import URL\n",
    "\n",
    "\n",
    "load_dotenv(\".env\", override=True)\n",
    "\n",
    "requests = aiohttp.ClientSession()\n",
    "\n",
    "\n",
    "async def get_text_Article(url: str) -> (str, str, str):\n",
    "    \"\"\"\n",
    "    Fetch the text of an article using Diffbot's Article API.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL of the article.\n",
    "\n",
    "    Returns:\n",
    "        (str, str, str) : title, language, text\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\" params = {\n",
    "        \"token\": getenv(\"DIFFBOT_API_KEY\"),\n",
    "        \"url\": url,\n",
    "    } \"\"\"\n",
    "    # params = \"token={}&url={}\".format(getenv(\"DIFFBOT_API_KEY\"), quote(url, safe=\":/\"))\n",
    "    params = \"?url={}&token={}\".format(quote(url, safe=\"\"), getenv(\"DIFFBOT_API_KEY\"))\n",
    "\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "    \"\"\" \n",
    "    Two hours of work to find out that the URL was encoded twice.\n",
    "    Diffbot doesn't accept / in the params. But aiohttp leaves them unencoded.\n",
    "    And any attempts to pre encode the URL with quote() would result to escaping twice the url.\n",
    "     \n",
    "    \"\"\"\n",
    "\n",
    "    url = URL(\"https://api.diffbot.com/v3/article\"+params, encoded=True)\n",
    "\n",
    "    response = await requests.get(\n",
    "        url, headers=headers)\n",
    "    \n",
    "\n",
    "    # We check if the request was successful.\n",
    "    if (response.status != 200):\n",
    "        raise Exception(\"Error while fetching the text of the article. Status code: {}\".format(\n",
    "            response.status))\n",
    "\n",
    "    data = (await response.json())\n",
    "\n",
    "    if \"error\" in data:\n",
    "        raise Exception(\"Error {} while fetching the text of the article: {}\".format(data[\"errorCode\"],\n",
    "            data[\"error\"]))\n",
    "\n",
    "    data = data[\"objects\"][0]\n",
    "\n",
    "    # We check if the text is returned by the API.\n",
    "    if (\"text\" not in data):\n",
    "        raise Exception(\"Error no text for the article: {}\".format(\n",
    "            data[\"error\"]))\n",
    "\n",
    "    return data[\"title\"], data[\"humanLanguage\"], data[\"text\"]\n",
    "\n",
    "\n",
    "def set_null_in_database(id: int):\n",
    "    \"\"\"\n",
    "    Set the text of the article to NULL in the database.\n",
    "\n",
    "    Args:\n",
    "        id (int): The id of the article.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cur = conn.cursor()\n",
    "\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO hn_article (id, title, language, text) VALUES (%s, NULL, NULL, NULL);\"\"\", (id,))\n",
    "\n",
    "        conn.commit()\n",
    "\n",
    "        cur.close()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        conn.rollback()\n",
    "\n",
    "\n",
    "async def addArticleToDatabase(params: (int, str)):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        params (int, str): The id and the url of the article.\n",
    "    \"\"\"\n",
    "\n",
    "    id, url = params\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "\n",
    "    # These websites are not supported by Diffbot.\n",
    "    if parsed.hostname == \"www.youtube.com\" or parsed.hostname == \"youtu.be\" or parsed.hostname == \"youtube.com\" or parsed.hostname == \"twitter.com\" or parsed.hostname == \"x.com\":\n",
    "        set_null_in_database(id)\n",
    "        print(\"Unsupported website ({})\".format(id))\n",
    "\n",
    "    if parsed.hostname == \"arxiv.org\":\n",
    "        documentCode = url.split(\"/\")[-1]\n",
    "        url = \"https://arxiv.org/pdf/{}.pdf\".format(documentCode)\n",
    "\n",
    "    try:\n",
    "        title, language, text = await get_text_Article(url)\n",
    "        if text == \"\":\n",
    "            raise Exception(\"Empty text for article {}\".format(id))\n",
    "    except Exception as e:\n",
    "        print(\"Error\", id, str(e), url)\n",
    "        set_null_in_database(id)\n",
    "        return\n",
    "\n",
    "    cur = conn.cursor()\n",
    "    try:\n",
    "        cur.execute(\"\"\"\n",
    "        INSERT INTO hn_article (id, title, language, text)\n",
    "        VALUES (%s, %s, %s, %s);\"\"\", (id, title, language, text))\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "    cur.close()\n",
    "\n",
    "    print(\"Success ({})\".format(id))\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success (38968269)\n",
      "Success (38968550)\n",
      "Success (38965509)\n",
      "Success (38967262)\n",
      "Success (38965306)\n",
      "Success (38977128)\n",
      "Success (38971966)\n",
      "Unsupported website (38966306)\n",
      "Success (38966601)\n",
      "Success (38966306)\n",
      "Success (38975453)\n",
      "Success (38971178)\n",
      "Success (38964675)\n",
      "Success (38969985)\n",
      "Success (38972735)\n",
      "Success (38969348)\n",
      "Success (38976955)\n",
      "Success (38965310)\n",
      "Success (38966145)\n",
      "Success (38975204)\n",
      "Success (38969114)\n",
      "Success (38975226)\n",
      "Success (38969759)\n",
      "Success (38966875)\n",
      "Success (38969461)\n",
      "Success (38977692)\n",
      "Success (38978705)\n",
      "Success (38972362)\n",
      "Success (38963678)\n",
      "Success (38964958)\n",
      "Success (38964983)\n",
      "Error 38971012 Error 500 while fetching the text of the article: Error processing page. https://www.amazon.com/fulfill-request-respectful-information-users-Brown/dp/B0CM82FJL2\n",
      "Success (38976254)\n",
      "Success (38968619)\n",
      "Success (38971221)\n",
      "Success (38978665)\n",
      "Unsupported website (38974802)\n",
      "Success (38972358)\n",
      "Success (38965003)\n",
      "Success (38978289)\n",
      "Unsupported website (38968359)\n",
      "Success (38969893)\n",
      "Success (38974802)\n",
      "Success (38967744)\n",
      "Error 38968359 Empty text for article 38968359 https://www.youtube.com/watch?v=f3hLZCuh8yM\n",
      "duplicate key value violates unique constraint \"hn_article_pkey\"\n",
      "DETAIL:  Key (id)=(38968359) already exists.\n",
      "\n",
      "Success (38967684)\n",
      "Success (38969072)\n",
      "Success (38964972)\n",
      "Success (38972153)\n",
      "Success (38966035)\n",
      "Success (38977532)\n",
      "Success (38971811)\n",
      "Success (38969466)\n",
      "Success (38980171)\n",
      "Success (38974404)\n",
      "Error 38978503 Error 500 while fetching the text of the article: Error processing page. https://www.inc.com/suzanne-lucas/viral-cloudflare-termination-video-masterclass-how-not-terminate.html\n"
     ]
    }
   ],
   "source": [
    "import aiometer\n",
    "\n",
    "await aiometer.run_on_each(addArticleToDatabase, posts, max_per_second=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
